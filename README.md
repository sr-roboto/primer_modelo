# Analizador de Toxicidad

Una aplicaciÃ³n web que permite analizar textos para detectar contenido
potencialmente tÃ³xico utilizando modelos de aprendizaje automÃ¡tico de
TensorFlow.

## ğŸ“‹ CaracterÃ­sticas

- Interfaz de usuario moderna y responsive
- AnÃ¡lisis en tiempo real de texto
- DetecciÃ³n de varios tipos de toxicidad (insultos, amenazas, lenguaje obsceno,
  etc.)
- VisualizaciÃ³n clara de resultados con porcentajes de probabilidad
- Indicador visual de la gravedad del contenido tÃ³xico

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Frontend**: HTML5, CSS3, JavaScript
- **Backend**: Node.js, Express.js
- **Modelo ML**: TensorFlow.js, @tensorflow-models/toxicity
- **Herramientas**: Nodemon para desarrollo

## âš™ï¸ InstalaciÃ³n

1. Clona este repositorio:

   ```bash
   git clone https://github.com/sr-roboto/primer_modelo.git
   cd primer_modelo
   ```

2. Instala las dependencias:

   ```bash
   npm install
   ```

3. Inicia el servidor de desarrollo:

   ```bash
   npm run dev
   ```

4. Abre tu navegador y visita `http://localhost:3000`

## ğŸš€ Uso

1. Ingresa un texto en el campo de entrada
2. Haz clic en el botÃ³n "Analizar" o presiona Enter
3. Espera mientras el modelo procesa el texto
4. Revisa los resultados del anÃ¡lisis:
   - Verde: No se detectÃ³ contenido tÃ³xico
   - Amarillo: Contenido potencialmente tÃ³xico (nivel medio)
   - Rojo: Contenido altamente tÃ³xico

## ğŸ“ Estructura del Proyecto

```
primer_modelo/
â”œâ”€â”€ src/                  # CÃ³digo fuente
â”‚   â”œâ”€â”€ index.js          # Servidor Express
â”‚   â”œâ”€â”€ public/           # Archivos estÃ¡ticos
â”‚   â”‚   â”œâ”€â”€ index.html    # PÃ¡gina principal
â”‚   â”‚   â”œâ”€â”€ scripts/      # JavaScript del cliente
â”‚   â”‚   â”‚   â””â”€â”€ toxicity.js
â”‚   â”‚   â””â”€â”€ styles/       # Hojas de estilo CSS
â”‚   â”‚       â””â”€â”€ home.css
â”œâ”€â”€ package.json          # Dependencias y scripts
â””â”€â”€ README.md             # DocumentaciÃ³n
```

## ğŸ“¡ API

### Endpoints

- `GET /`: PÃ¡gina principal
- `POST /submit`: Analiza el texto para detectar toxicidad
  - Body: `{ "text": "texto a analizar" }`
  - Respuesta: Lista de tipos de toxicidad detectados o mensaje de no toxicidad

### Ejemplo de respuesta

```json
[
  {
    "label": "identity_attack",
    "results": [{ "match": true, "probabilities": [0.1, 0.9] }]
  },
  {
    "label": "insult",
    "results": [{ "match": true, "probabilities": [0.2, 0.8] }]
  }
]
```

## âœï¸ Autor

**Marcos Lopez** - [sr-roboto](https://github.com/sr-roboto)

```

```
